# -*- coding: utf-8 -*-
"""Untitled18.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oeU4ZT0yrJWsG6vwcEO4mDquGBq3JVOn
"""

import cv2
import math
import numpy as np
import time

# Define the dehazing functions from the original code

def DarkChannel(im,sz):
    b,g,r = cv2.split(im)
    dc = cv2.min(cv2.min(r,g),b);
    kernel = cv2.getStructuringElement(cv2.MORPH_RECT,(sz,sz))
    dark = cv2.erode(dc,kernel)
    return dark

def AtmLight(im,dark):
    [h,w] = im.shape[:2]
    imsz = h*w
    numpx = int(max(math.floor(imsz/1000),1))
    darkvec = dark.reshape(imsz);
    imvec = im.reshape(imsz,3);

    indices = darkvec.argsort();
    indices = indices[imsz-numpx::]

    atmsum = np.zeros([1,3])
    for ind in range(1,numpx):
       atmsum = atmsum + imvec[indices[ind]]

    A = atmsum / numpx;
    return A

def TransmissionEstimate(im,A,sz):
    omega = 0.95;
    im3 = np.empty(im.shape,im.dtype);

    for ind in range(0,3):
        im3[:,:,ind] = im[:,:,ind]/A[0,ind]

    transmission = 1 - omega*DarkChannel(im3,sz);
    return transmission

def Guidedfilter(im,p,r,eps):
    mean_I = cv2.boxFilter(im,cv2.CV_64F,(r,r));
    mean_p = cv2.boxFilter(p, cv2.CV_64F,(r,r));
    mean_Ip = cv2.boxFilter(im*p,cv2.CV_64F,(r,r));
    cov_Ip = mean_Ip - mean_I*mean_p;

    mean_II = cv2.boxFilter(im*im,cv2.CV_64F,(r,r));
    var_I   = mean_II - mean_I*mean_I;

    a = cov_Ip/(var_I + eps);
    b = mean_p - a*mean_I;

    mean_a = cv2.boxFilter(a,cv2.CV_64F,(r,r));
    mean_b = cv2.boxFilter(b,cv2.CV_64F,(r,r));

    q = mean_a*im + mean_b;
    return q;

def TransmissionRefine(im,et):
    gray = cv2.cvtColor(im,cv2.COLOR_BGR2GRAY);
    gray = np.float64(gray)/255;
    r = 60;
    eps = 0.0001;
    t = Guidedfilter(gray,et,r,eps);

    return t;

def Recover(im,t,A,tx = 0.1):
    res = np.empty(im.shape,im.dtype);
    t = cv2.max(t,tx);

    for ind in range(0,3):
        res[:,:,ind] = (im[:,:,ind]-A[0,ind])/t + A[0,ind]

    return res





# Function to annotate frames
def annotate_frame(frame, haze_detected):
    font = cv2.FONT_HERSHEY_SIMPLEX
    font_scale = 0.5
    font_color = (0, 0, 255)  # Red color
    font_thickness = 1
    y0, dy = 25, 20  # Text position

    # Annotate with haze information
    if haze_detected:
        text = "Haze Detected"
    else:
        text = "No Haze"

    cv2.putText(frame, text, (10, y0), font, font_scale, font_color, font_thickness, lineType=cv2.LINE_AA)

# Function to extract metadata (you can customize this)
def extract_metadata(frame):
    timestamp = time.time()
    weather_conditions = "Sunny"
    environmental_data = "Temperature: 25°C, Humidity: 50%"

    return f"Timestamp: {timestamp}\nWeather: {weather_conditions}\nEnvironment: {environmental_data}"

if __name__ == '__main__':
    try:
        input_video = '/content/sEmptyR1.avi'  # Change to your video file path
        output_video = 'output_annotated_video.avi'  # Specify the output video file
    except:
        input_video = './image/15.png'

    cap = cv2.VideoCapture(input_video)
    fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Specify the codec for the output video
    out = cv2.VideoWriter(output_video, fourcc, 30.0, (int(cap.get(3)), int(cap.get(4))))

    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break

        # Perform dehazing on the frame
        I = frame.astype('float64') / 255
        dark = DarkChannel(I, 15)
        A = AtmLight(I, dark)
        te = TransmissionEstimate(I, A, 15)
        t = TransmissionRefine(frame, te)
        dehazed_frame = Recover(I, t, A, 0.1)

        # Detect haze by checking the dark channel or other suitable criteria
        haze_detected = np.max(dark) > 100  # Adjust the threshold as needed

        # Perform annotation on the frame
        annotation_text = extract_metadata(frame)
        annotate_frame(dehazed_frame, haze_detected)

        # Write the annotated frame to the output video
        out.write(dehazed_frame)

        # Display the annotated frame (optional)
        cv2.imshow("Annotated Frame", dehazed_frame)

        # Press 'q' to exit the video playback
        if cv2.waitKey(1) & 0xFF == ord('q'):
            break

    cap.release()
    out.release()
    cv2.destroyAllWindows()

import cv2
import math
import numpy as np
import time
from google.colab.patches import cv2_imshow






# Define the dehazing functions from the original code

def DarkChannel(im, sz):
    b, g, r = cv2.split(im)
    dc = cv2.min(cv2.min(r, g), b)
    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (sz, sz))
    dark = cv2.erode(dc, kernel)
    return dark

def AtmLight(im, dark):
    [h, w] = im.shape[:2]
    imsz = h * w
    numpx = int(max(math.floor(imsz / 1000), 1))
    darkvec = dark.reshape(imsz)
    imvec = im.reshape(imsz, 3)

    indices = darkvec.argsort()
    indices = indices[imsz - numpx::]

    atmsum = np.zeros([1, 3])
    for ind in range(1, numpx):
        atmsum = atmsum + imvec[indices[ind]]

    A = atmsum / numpx
    return A

def TransmissionEstimate(im, A, sz):
    omega = 0.95
    im3 = np.empty(im.shape, im.dtype)

    for ind in range(0, 3):
        im3[:, :, ind] = im[:, :, ind] / A[0, ind]

    transmission = 1 - omega * DarkChannel(im3, sz)
    return transmission

def Guidedfilter(im, p, r, eps):
    mean_I = cv2.boxFilter(im, cv2.CV_64F, (r, r))
    mean_p = cv2.boxFilter(p, cv2.CV_64F, (r, r))
    mean_Ip = cv2.boxFilter(im * p, cv2.CV_64F, (r, r))
    cov_Ip = mean_Ip - mean_I * mean_p

    mean_II = cv2.boxFilter(im * im, cv2.CV_64F, (r, r))
    var_I = mean_II - mean_I * mean_I

    a = cov_Ip / (var_I + eps)
    b = mean_p - a * mean_I

    mean_a = cv2.boxFilter(a, cv2.CV_64F, (r, r))
    mean_b = cv2.boxFilter(b, cv2.CV_64F, (r, r))

    q = mean_a * im + mean_b
    return q

def TransmissionRefine(im, et):
    gray = cv2.cvtColor(im, cv2.COLOR_BGR2GRAY)
    gray = np.float64(gray) / 255
    r = 60
    eps = 0.0001
    t = Guidedfilter(gray, et, r, eps)

    return t

def Recover(im, t, A, tx=0.1):
    res = np.empty(im.shape, im.dtype)
    t = cv2.max(t, tx)

    for ind in range(0, 3):
        res[:, :, ind] = (im[:, :, ind] - A[0, ind]) / t + A[0, ind]

    return res

# Function to annotate frames
def annotate_frame(frame, haze_detected):
    font = cv2.FONT_HERSHEY_SIMPLEX
    font_scale = 0.5
    font_color = (0, 0, 255)  # Red color
    font_thickness = 1
    y0, dy = 25, 20  # Text position

    # Annotate with haze information
    if haze_detected:
        text = "Haze Detected"
    else:
        text = "No Haze"

    cv2.putText(frame, text, (10, y0), font, font_scale, font_color, font_thickness, lineType=cv2.LINE_AA)

# Function to extract metadata (you can customize this)
def extract_metadata(frame):
    timestamp = time.time()
    weather_conditions = "Sunny"
    environmental_data = "Temperature: 25°C, Humidity: 50%"

    return f"Timestamp: {timestamp}\nWeather: {weather_conditions}\nEnvironment: {environmental_data}"

if __name__ == '__main__':
    try:
        input_video = '/content/sEmptyR1.avi'  # Change to your video file path
        output_video = 'output_annotated_video.avi'  # Specify the output video file
    except:
        input_video = './image/15.png'

    cap = cv2.VideoCapture(input_video)
    fourcc = cv2.VideoWriter_fourcc(*'XVID')  # Specify the codec for the output video (changed to XVID)
    out = cv2.VideoWriter(output_video, fourcc, 30.0, (int(cap.get(3)), int(cap.get(4))))

    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break

        # Perform dehazing on the frame
        I = frame.astype('float64') / 255
        dark = DarkChannel(I, 15)
        A = AtmLight(I, dark)
        te = TransmissionEstimate(I, A, 15)
        t = TransmissionRefine(frame, te)
        dehazed_frame = Recover(I, t, A, 0.1)

        # Detect haze by checking the dark channel or other suitable criteria
        haze_detected = np.max(dark) > 100  # Adjust the threshold as needed

        # Perform annotation on the frame
        annotation_text = extract_metadata(frame)
        annotate_frame(dehazed_frame, haze_detected)

        # Convert the dehazed_frame to CV_8U data type
        dehazed_frame_uint8 = cv2.convertScaleAbs(dehazed_frame * 255)

        # Write the annotated frame to the output video
        cv2_imshow(dehazed_frame_uint8)

        # Display the annotated frame (optional)
        cv2.imshow("Annotated Frame", dehazed_frame_uint8)

        # Press 'q' to exit the video playback
        if cv2.waitKey(1) & 0xFF == ord('q'):
            break

    cap.release()
    out.release()
    cv2.destroyAllWindows()

import cv2
import math
import numpy as np
import time
from google.colab.patches import cv2_imshow

# Define the dehazing functions (similar to previous code)

# Function to annotate frames (add your annotations here)
def annotate_frame(frame, haze_detected, metadata):
    font = cv2.FONT_HERSHEY_SIMPLEX
    font_scale = 0.5
    font_color = (0, 0, 255) if haze_detected else (0, 255, 0)  # Red for haze, green for non-haze
    font_thickness = 1
    y0, dy = 25, 20  # Text position

    # Annotate with haze information and metadata
    text = "Haze Detected" if haze_detected else "No Haze"
    text += f"\n{metadata}"  # Add metadata here

    cv2.putText(frame, text, (10, y0), font, font_scale, font_color, font_thickness, lineType=cv2.LINE_AA)

if __name__ == '__main__':
    try:
        input_video = '/content/sEmptyR1.avi'  # Change to your video file path
        output_video = 'output_annotated_video.avi'  # Specify the output video file
    except:
        input_video = './image/15.png'

    cap = cv2.VideoCapture(input_video)
    fourcc = cv2.VideoWriter_fourcc(*'XVID')  # Specify the codec for the output video (changed to XVID)
    out = cv2.VideoWriter(output_video, fourcc, 30.0, (int(cap.get(3)), int(cap.get(4))))

    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break

        # Perform dehazing on the frame
        I = frame.astype('float64') / 255
        dark = DarkChannel(I, 15)
        A = AtmLight(I, dark)
        te = TransmissionEstimate(I, A, 15)
        t = TransmissionRefine(frame, te)
        dehazed_frame = Recover(I, t, A, 0.1)

        # Detect haze by checking the dark channel or other suitable criteria
        haze_detected = np.max(dark) > 100  # Adjust the threshold as needed

        # Extract metadata from the frame (customize this function)
        metadata = extract_metadata(frame)

        # Annotate and display the frame (optional)
        annotate_frame(dehazed_frame, haze_detected, metadata)
        cv2_imshow(dehazed_frame)

        # Write the annotated frame to the output video
        out.write(dehazed_frame)

        # Press 'q' to exit the video playback
        if cv2.waitKey(1) & 0xFF == ord('q'):
            break

    cap.release()
    out.release()
    cv2.destroyAllWindows()

import cv2
import math
import numpy as np
import time
from google.colab.patches import cv2_imshow

# Define the dehazing functions from the original code

def DarkChannel(im, sz):
    b, g, r = cv2.split(im)
    dc = cv2.min(cv2.min(r, g), b)
    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (sz, sz))
    dark = cv2.erode(dc, kernel)
    return dark

def AtmLight(im, dark):
    [h, w] = im.shape[:2]
    imsz = h * w
    numpx = int(max(math.floor(imsz / 1000), 1))
    darkvec = dark.reshape(imsz)
    imvec = im.reshape(imsz, 3)

    indices = darkvec.argsort()
    indices = indices[imsz - numpx::]

    atmsum = np.zeros([1, 3])
    for ind in range(1, numpx):
        atmsum = atmsum + imvec[indices[ind]]

    A = atmsum / numpx
    return A

def TransmissionEstimate(im, A, sz):
    omega = 0.95
    im3 = np.empty(im.shape, im.dtype)

    for ind in range(0, 3):
        im3[:, :, ind] = im[:, :, ind] / A[0, ind]

    transmission = 1 - omega * DarkChannel(im3, sz)
    return transmission

def Guidedfilter(im, p, r, eps):
    mean_I = cv2.boxFilter(im, cv2.CV_64F, (r, r))
    mean_p = cv2.boxFilter(p, cv2.CV_64F, (r, r))
    mean_Ip = cv2.boxFilter(im * p, cv2.CV_64F, (r, r))
    cov_Ip = mean_Ip - mean_I * mean_p

    mean_II = cv2.boxFilter(im * im, cv2.CV_64F, (r, r))
    var_I = mean_II - mean_I * mean_I

    a = cov_Ip / (var_I + eps)
    b = mean_p - a * mean_I

    mean_a = cv2.boxFilter(a, cv2.CV_64F, (r, r))
    mean_b = cv2.boxFilter(b, cv2.CV_64F, (r, r))

    q = mean_a * im + mean_b
    return q

def TransmissionRefine(im, et):
    gray = cv2.cvtColor(im, cv2.COLOR_BGR2GRAY)
    gray = np.float64(gray) / 255
    r = 60
    eps = 0.0001
    t = Guidedfilter(gray, et, r, eps)

    return t

def Recover(im, t, A, tx=0.1):
    res = np.empty(im.shape, im.dtype)
    t = cv2.max(t, tx)

    for ind in range(0, 3):
        res[:, :, ind] = (im[:, :, ind] - A[0, ind]) / t + A[0, ind]

    return res

# Function to annotate frames
# Function to annotate frames
def annotate_frame(frame, haze_detected, metadata):
    font = cv2.FONT_HERSHEY_SIMPLEX
    font_scale = 0.5
    font_thickness = 1
    y0, dy = 25, 20  # Text position

    if haze_detected:
        font_color = (0, 0, 255)  # Red color
        text = "Haze Detected"
    else:
        font_color = (0, 255, 0)  # Green color
        text = "No Haze"

    # Annotate with metadata
    text += f"\n{metadata}"

    cv2.putText(frame, text, (10, y0), font, font_scale, font_color, font_thickness, lineType=cv2.LINE_AA)


# Function to extract metadata (you can customize this)
def extract_metadata(frame):
    timestamp = time.time()
    weather_conditions = "Sunny"
    environmental_data = "Temperature: 25°C, Humidity: 50%"

    return f"Timestamp: {timestamp}\nWeather: {weather_conditions}\nEnvironment: {environmental_data}"

if __name__ == '__main__':
    try:
        input_video = '/content/sEmptyR1.avi'  # Change to your video file path
        output_video = '/content/output_annotated_video.avi'  # Specify the output video file path in Colab
    except:
        input_video = '/content/image/15.png'

    cap = cv2.VideoCapture(input_video)
    fourcc = cv2.VideoWriter_fourcc(*'XVID')  # Specify the codec for the output video (changed to XVID)
    out = cv2.VideoWriter(output_video, fourcc, 30.0, (int(cap.get(3)), int(cap.get(4))))

    frame_number = 0  # Initialize frame number

    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break

        frame_number += 1  # Increment frame number

        # Perform dehazing on the frame
        I = frame.astype('float64') / 255
        dark = DarkChannel(I, 15)
        A = AtmLight(I, dark)
        te = TransmissionEstimate(I, A, 15)
        t = TransmissionRefine(frame, te)
        dehazed_frame = Recover(I, t, A, 0.1)

        # Detect haze by checking the dark channel or other suitable criteria
        haze_detected = np.max(dark) > 100  # Adjust the threshold as needed

        # Extract metadata from the frame
        metadata = extract_metadata(frame)

        # Perform annotation on the frame
        annotate_frame(dehazed_frame, haze_detected, metadata)

        # Convert the dehazed_frame to CV_8U data type
        dehazed_frame_uint8 = cv2.convertScaleAbs(dehazed_frame * 255)

        # Write the annotated frame to the output video
        out.write(dehazed_frame_uint8)

        # Display the annotated frame (optional)
        cv2_imshow(dehazed_frame_uint8)

        # Press 'q' to exit the video playback
        if cv2.waitKey(1) & 0xFF == ord('q'):
            break

    cap.release()
    out.release()
    cv2.destroyAllWindows()

import cv2
import math
import numpy as np
import time
from google.colab.patches import cv2_imshow

# Define the dehazing functions from the original code
# ...
def HazeDetectionHistogram(im):
    # Convert the image to grayscale
    gray = cv2.cvtColor(im, cv2.COLOR_BGR2GRAY)

    # Calculate the histogram of the grayscale image
    hist = cv2.calcHist([gray], [0], None, [256], [0, 256])

    # Normalize the histogram to [0, 1]
    hist /= hist.sum()

    # Calculate the cumulative distribution function (CDF) of the histogram
    cdf = hist.cumsum()

    # Calculate the percentage of bright pixels (e.g., top 5%) in the image
    bright_pixel_percentage = 100 - (cdf[int(0.95 * 255)] * 100)

    # Set a threshold value (adjust as needed)
    threshold_percentage = 2.0  # Adjust this threshold value as needed

    # Determine haze presence based on the threshold
    haze_detected = bright_pixel_percentage >= threshold_percentage

    return haze_detected

# Function to extract timestamps from each frame
def extract_timestamp(frame, cap):
    frame_number = int(cap.get(cv2.CAP_PROP_POS_FRAMES))
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    frame_timestamp = frame_number / total_frames * cap.get(cv2.CAP_PROP_POS_MSEC) / 1000.0
    return frame_timestamp

# ...

if __name__ == '__main__':
    try:
        input_video = '/content/sEmptyR1.avi'  # Change to your video file path
        output_video = 'output_annotated_video.avi'  # Specify the output video file
    except:
        input_video = './image/15.png'

    cap = cv2.VideoCapture(input_video)
    fourcc = cv2.VideoWriter_fourcc(*'XVID')  # Specify the codec for the output video (changed to XVID)
    out = cv2.VideoWriter(output_video, fourcc, 30.0, (int(cap.get(3)), int(cap.get(4))))

    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break

        # Extract the frame timestamp
        frame_timestamp = extract_timestamp(frame, cap)

        # Perform haze detection using histogram analysis
        haze_detected = HazeDetectionHistogram(frame)

        # Perform annotation on the frame
        annotation_text = f"Timestamp: {frame_timestamp:.2f} sec\n"
        annotation_text += extract_metadata(frame)
        annotate_frame(frame, haze_detected, annotation_text)

        # Convert the frame to CV_8U data type
        frame_uint8 = cv2.convertScaleAbs(frame * 255)

        # Write the annotated frame to the output video
        out.write(frame_uint8)

        # Display the annotated frame (optional)
        cv2_imshow(frame_uint8)

        # Press 'q' to exit the video playback
        if cv2.waitKey(1) & 0xFF == ord('q'):
            break

    cap.release()
    out.release()
    cv2.destroyAllWindows()

import math
import numpy as np
import time
from google.colab.patches import cv2_imshow
import cv2

# Define the dehazing functions from the original code

# ...
# Function to annotate frames


def DarkChannel(im, sz):
    b, g, r = cv2.split(im)
    dc = cv2.min(cv2.min(r, g), b)
    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (sz, sz))
    dark = cv2.erode(dc, kernel)
    return dark

def AtmLight(im, dark):
    [h, w] = im.shape[:2]
    imsz = h * w
    numpx = int(max(math.floor(imsz / 1000), 1))
    darkvec = dark.reshape(imsz)
    imvec = im.reshape(imsz, 3)

    indices = darkvec.argsort()
    indices = indices[imsz - numpx::]

    atmsum = np.zeros([1, 3])
    for ind in range(1, numpx):
        atmsum = atmsum + imvec[indices[ind]]

    A = atmsum / numpx
    return A

def TransmissionEstimate(im, A, sz):
    omega = 0.95
    im3 = np.empty(im.shape, im.dtype)

    for ind in range(0, 3):
        im3[:, :, ind] = im[:, :, ind] / A[0, ind]

    transmission = 1 - omega * DarkChannel(im3, sz)
    return transmission

def Guidedfilter(im, p, r, eps):
    mean_I = cv2.boxFilter(im, cv2.CV_64F, (r, r))
    mean_p = cv2.boxFilter(p, cv2.CV_64F, (r, r))
    mean_Ip = cv2.boxFilter(im * p, cv2.CV_64F, (r, r))
    cov_Ip = mean_Ip - mean_I * mean_p

    mean_II = cv2.boxFilter(im * im, cv2.CV_64F, (r, r))
    var_I = mean_II - mean_I * mean_I

    a = cov_Ip / (var_I + eps)
    b = mean_p - a * mean_I

    mean_a = cv2.boxFilter(a, cv2.CV_64F, (r, r))
    mean_b = cv2.boxFilter(b, cv2.CV_64F, (r, r))

    q = mean_a * im + mean_b
    return q

def TransmissionRefine(im, et):
    gray = cv2.cvtColor(im, cv2.COLOR_BGR2GRAY)
    gray = np.float64(gray) / 255
    r = 60
    eps = 0.0001
    t = Guidedfilter(gray, et, r, eps)

    return t

def Recover(im, t, A, tx=0.1):
    res = np.empty(im.shape, im.dtype)
    t = cv2.max(t, tx)

    for ind in range(0, 3):
        res[:, :, ind] = (im[:, :, ind] - A[0, ind]) / t + A[0, ind]

    return res
def HazeDetectionHistogram(im):
    # Convert the image to grayscale
    gray = cv2.cvtColor(im, cv2.COLOR_BGR2GRAY)

    # Calculate the histogram of the grayscale image
    hist = cv2.calcHist([gray], [0], None, [256], [0, 256])

    # Normalize the histogram to [0, 1]
    hist /= hist.sum()

    # Calculate the cumulative distribution function (CDF) of the histogram
    cdf = hist.cumsum()

    # Calculate the percentage of bright pixels (e.g., top 5%) in the image
    bright_pixel_percentage = 100 - (cdf[int(0.95 * 255)] * 100)

    # Set a threshold value (adjust as needed)
    threshold_percentage = 2.0  # Adjust this threshold value as needed

    # Determine haze presence based on the threshold
    haze_detected = bright_pixel_percentage >= threshold_percentage

    return haze_detected
def extract_timestamp(frame, cap):
    frame_number = int(cap.get(cv2.CAP_PROP_POS_FRAMES))
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    frame_timestamp = frame_number / total_frames * cap.get(cv2.CAP_PROP_POS_MSEC) / 1000.0
    return frame_timestamp

# Function to create a visualization of regions with and without haze
def visualize_haze_regions(frame, haze_detected):
    visualization = frame.copy()

    if haze_detected:
        # Highlight regions with haze by applying a blue tint
        blue_tint = np.array([100, 0, 0], dtype=np.uint8)
        visualization = cv2.addWeighted(visualization, 1.0, blue_tint, 0.5, 0)

    return visualization
# Function to annotate frames
def annotate_frame(frame, haze_detected, annotation_text):
    font = cv2.FONT_HERSHEY_SIMPLEX
    font_scale = 0.5
    font_color = (0, 0, 255)  # Red color
    font_thickness = 1
    y0, dy = 25, 20  # Text position

    # Annotate with haze information
    if haze_detected:
        text = "Haze Detected"
    else:
        text = "No Haze"

    cv2.putText(frame, text, (10, y0), font, font_scale, font_color, font_thickness, lineType=cv2.LINE_AA)

    # Add the custom annotation text
    cv2.putText(frame, annotation_text, (10, y0 + dy), font, font_scale, font_color, font_thickness, lineType=cv2.LINE_AA)

# Function to extract metadata (you can customize this)
# ...
# Function to extract metadata (you can customize this)
def extract_metadata(frame):
    timestamp = time.time()
    weather_conditions = "Sunny"
    environmental_data = "Temperature: 25°C, Humidity: 50%"

    return f"Timestamp: {timestamp}\nWeather: {weather_conditions}\nEnvironment: {environmental_data}"
if __name__ == '__main__':
    try:
        input_video = '/content/sEmptyR1.avi'  # Change to your video file path
        output_video = 'output_annotated_video.avi'  # Specify the output video file
    except:
        input_video = './image/15.png'

    cap = cv2.VideoCapture(input_video)
    fourcc = cv2.VideoWriter_fourcc(*'XVID')  # Specify the codec for the output video (changed to XVID)
    out = cv2.VideoWriter(output_video, fourcc, 30.0, (int(cap.get(3)), int(cap.get(4))))

    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break

        # Extract the frame timestamp
        frame_timestamp = extract_timestamp(frame, cap)

        # Perform haze detection using histogram analysis
        haze_detected = HazeDetectionHistogram(frame)

        # Perform annotation on the frame
        annotation_text = f"Timestamp: {frame_timestamp:.2f} sec\n"
        annotation_text += extract_metadata(frame)
        annotate_frame(frame, haze_detected, annotation_text)

        # Convert the frame to CV_8U data type
        frame_uint8 = cv2.convertScaleAbs(frame * 255)

        # Write the annotated frame to the output video
        out.write(frame_uint8)

        # Display the annotated frame (optional)
        cv2_imshow(frame_uint8)

        # Press 'q' to exit the video playback
        if cv2.waitKey(1) & 0xFF == ord('q'):
            break

    cap.release()
    out.release()
    cv2.destroyAllWindows()

